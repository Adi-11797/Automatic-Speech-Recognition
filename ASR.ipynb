{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HW3P2: Utterance to Phoneme Mapping\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this homework, you would ideally have learned:\n",
        "\n",
        "• To solve a sequence-to-sequence problem using Sequence models.\n",
        "    – How to set up GRU/LSTM based models on pytorch\n",
        "    – How to utilize CNNs as feature extractors\n",
        "    – How to handle sequential data\n",
        "    – How to pad / pack baches of variable length data\n",
        "    – How to train the model using CTC Loss\n",
        "    – How to optimize the model\n",
        "    – How to implement and utilize decoders such as greedy and beam decoders\n",
        "\n",
        "• To explore architectures and hyperparameters for the optimal solution\n",
        "– To identify and tabulate all the various design/architecture choices, parameters\n",
        "and hyperparameters that affect your solution\n",
        "– To devise strategies to search through this space of options to find the best solution\n",
        "\n",
        "• The process of staging the exploration\n",
        "– To initially set up a simple solution that is easily implemented and optimized\n",
        "– To stage your data to efficiently search through the space of solutions\n",
        "– To subset promising configurations/settings and tune them to obtain higher performance\n",
        "\n",
        "• To engineer the solution using your tools\n",
        "– To use objects from the PyTorch framework to build a GRU/LSTM based model\n",
        "– To deal with issues of data loading, memory usage, arithmetic precision etc. to maximize the time efficiency of your training and inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# README"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instructions to run code: All cells need to be run! \n",
        "(Note: Training cell was run twice to reach high cutoff - at the time of submission I had re-run the cell (instead of seperately running the cell) due to a lack of time, so the output logs of the first training run are not visible. Moreover, the additional training was only run for 10 epochs)\n",
        "\n",
        "Ablations Strategies: \n",
        "\n",
        "1) Architectures considered:\n",
        "- Simplified 1D Convolutional layer + BatchNorm of LSTM  (Easy implementation - Low cutoff reached)\n",
        "- 2 1D-Conv layers with RELU (Allowed me to reach Medium Cutoff sucesfully, however, improvements thereafter were quite tedious and required a reconsideration of architecture)\n",
        "- 2 1D-Conv layers with GELU   (Best Performance - highly improved Levenshtein Distance accuracy achieved within 15 epochs)\n",
        "- Additionally, after a group discussiion and suggestion from TA's, introducing GELU with dropout in the classification layer seemed to really bolster performance, and convergence was quite easily achieved\n",
        "\n",
        "2) Epochs: \n",
        "- Trained for 35 epochs in total. Training for longer epochs improved performance, however, beyond a certain number of epochs (50), the tradeoff in performance and resource consumption was not beneficial.\n",
        "\n",
        "3) Hyperparameters: \n",
        "* Learning Rate tuning was not required, 0.002 LR achieved the high cutoff requirement\n",
        "* Batch Size was experimented at different values from 16-128. Finally a batch_size of 32 was selected due its acceptable performance for compute units consumed. Although lower batch sizes notably improved performance.\n",
        "* LockedDropout was implemented as recommended\n",
        "\n",
        "4) Data loading scheme:\n",
        "* No transforms were required to reach cutoff."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR4qfYrVoO4v"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd5aNaLVoR_g"
      },
      "source": [
        "## wandb\n",
        "\n",
        "You will need to fetch your api key from wandb.ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA9qZoIDcx-h",
        "outputId": "48a82cac-6b90-4e19-939a-4d7a4915caf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.9 MB 4.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 168 kB 91.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 67.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 166 kB 85.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 166 kB 84.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 162 kB 79.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 162 kB 66.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 158 kB 73.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 77.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 82.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 64.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 99.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 66.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 88.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 85.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 156 kB 84.9 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiDduMaDIARE",
        "outputId": "1655adfc-07e5-4b60-aaeb-8681ec785ea1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login(key=\"2178c9f0d96e90016c3d36bcccb07de5e0c51edc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "4s52yBOvICPZ",
        "outputId": "76a616ff-0266-405e-9cff-7eab5994c5ba"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(\n",
        "    name = \"early-submission\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
        "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"hw3p2-ablations\", ### Project should be created in your wandb account \n",
        "    # config = config ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONgAWhqdoYy-"
      },
      "source": [
        "## Levenshtein\n",
        "\n",
        "This may take a while"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS7a7xeEoaV9",
        "outputId": "b0812ba3-7b0a-4ee9-89c8-9de0fbe734eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.20.8-py3-none-any.whl (9.4 kB)\n",
            "Collecting Levenshtein==0.20.8\n",
            "  Downloading Levenshtein-0.20.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n",
            "  Downloading rapidfuzz-2.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 68.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.20.8 python-Levenshtein-0.20.8 rapidfuzz-2.13.2\n",
            "Cloning into 'ctcdecode'...\n",
            "remote: Enumerating objects: 1102, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 1102 (delta 16), reused 32 (delta 14), pack-reused 1063\u001b[K\n",
            "Receiving objects: 100% (1102/1102), 782.27 KiB | 6.74 MiB/s, done.\n",
            "Resolving deltas: 100% (529/529), done.\n",
            "Submodule 'third_party/ThreadPool' (https://github.com/progschj/ThreadPool.git) registered for path 'third_party/ThreadPool'\n",
            "Submodule 'third_party/kenlm' (https://github.com/kpu/kenlm.git) registered for path 'third_party/kenlm'\n",
            "Cloning into '/content/ctcdecode/third_party/ThreadPool'...\n",
            "remote: Enumerating objects: 82, done.        \n",
            "remote: Total 82 (delta 0), reused 0 (delta 0), pack-reused 82        \n",
            "Cloning into '/content/ctcdecode/third_party/kenlm'...\n",
            "remote: Enumerating objects: 14102, done.        \n",
            "remote: Counting objects: 100% (415/415), done.        \n",
            "remote: Compressing objects: 100% (289/289), done.        \n",
            "remote: Total 14102 (delta 127), reused 382 (delta 112), pack-reused 13687        \n",
            "Receiving objects: 100% (14102/14102), 5.89 MiB | 20.45 MiB/s, done.\n",
            "Resolving deltas: 100% (8007/8007), done.\n",
            "Submodule path 'third_party/ThreadPool': checked out '9a42ec1329f259a5f4881a291db1dcb8f2ad9040'\n",
            "Submodule path 'third_party/kenlm': checked out '35835f1ac4884126458ac89f9bf6dd9ccad561e0'\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=94c934df68d0e90261195c5a4761036d9dee073b183672b9f6d8cceac6167b76\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "/content/ctcdecode\n",
            "Processing /content/ctcdecode\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Building wheels for collected packages: ctcdecode\n",
            "  Building wheel for ctcdecode (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ctcdecode: filename=ctcdecode-1.0.3-cp37-cp37m-linux_x86_64.whl size=13258433 sha256=32ab7b268cb368d126acd1720851594f387a9ac87e2668939b2497b99ea06b0b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-koh6p1j5/wheels/da/bb/b4/233de9fd7927245208e27bcf688bf5680ae3f3874be2895eef\n",
            "Successfully built ctcdecode\n",
            "Installing collected packages: ctcdecode\n",
            "Successfully installed ctcdecode-1.0.3\n",
            "/content\n",
            "Collecting torchsummaryX\n",
            "  Downloading torchsummaryX-1.3.0-py3-none-any.whl (3.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.21.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.10.0+cu111)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torchsummaryX) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torchsummaryX) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torchsummaryX) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchsummaryX) (3.10.0.2)\n",
            "Installing collected packages: torchsummaryX\n",
            "Successfully installed torchsummaryX-1.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install python-Levenshtein\n",
        "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "!pip install wget\n",
        "%cd ctcdecode\n",
        "!pip install .\n",
        "%cd ..\n",
        "\n",
        "!pip install torchsummaryX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWVONJxCobPc"
      },
      "source": [
        "## imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78ZTCIXoof2f",
        "outputId": "146bc030-1c36-4e86-ff75-5b0344fc72d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import torchaudio.transforms as tat\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "# imports for decoding and distance calculation\n",
        "import ctcdecode\n",
        "import Levenshtein\n",
        "from ctcdecode import CTCBeamDecoder\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg3-yJ8tok34"
      },
      "source": [
        "# Kaggle Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdUelfGhom1m",
        "outputId": "a8c9ccba-574d-471f-ae79-76f1db572d0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting kaggle==1.5.8\n",
            "  Downloading kaggle-1.5.8.tar.gz (59 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▌                          | 10 kB 25.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 20 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 30 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 40 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 59 kB 3.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.8-py3-none-any.whl size=73275 sha256=535f0676eae547f7409c6775e330fff99197661fb2a35814fc82ccbfc5342aaf\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/f7/d8/c3902cacb7e62cb611b1ad343d7cc07f42f7eb76ae3a52f3d1\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.8\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\"ripcurl11\",\"key\":\"a924e45910075179ad325ad28d952008\"}') # TODO: Put your kaggle username & key here\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSjBwfXeoq4B",
        "outputId": "bc8e1374-ccdf-4c68-baa9-48a8e3a49ef3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading 11-785-f22-hw3p2.zip to /content\n",
            "100% 8.88G/8.88G [00:53<00:00, 109MB/s]\n",
            "100% 8.88G/8.88G [00:53<00:00, 178MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions download -c 11-785-f22-hw3p2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ruxWP60LCQA",
        "outputId": "8779a8c6-2dec-4771-fca4-10200e1a95ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11-785-f22-hw3p2.zip  ctcdecode  hw3p2\tsample_data  wandb\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "This will take a couple minutes, but you should see at least the following:\n",
        "11-785-f22-hw3p2.zip  ctcdecode  hw3p2\n",
        "'''\n",
        "!unzip -q 11-785-f22-hw3p2.zip\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9v5ewZDMpYA"
      },
      "source": [
        "# Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Cp-716IMZRd",
        "outputId": "c35715e5-30fa-4be9-ff54-dccca0c46875"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\n",
            "E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease' is no longer signed.\n",
            "W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\n",
            "E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease' is no longer signed.\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: www-browser: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: links2: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: elinks: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: links: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: lynx: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: w3m: not found\n",
            "xdg-open: no method available for opening 'https://accounts.google.com/o/oauth2/auth?client_id=564921029129.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fgd-ocaml-auth.appspot.com%2Foauth2callback&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force&state=pUdfmCSvS4iQTSUgRKEErc81JArgOOWqIZEkO4si9mk'\n",
            "/bin/sh: 1: firefox: not found\n",
            "/bin/sh: 1: google-chrome: not found\n",
            "/bin/sh: 1: chromium-browser: not found\n",
            "/bin/sh: 1: open: not found\n",
            "Cannot retrieve auth tokens.\n",
            "Failure(\"Error opening URL:https://accounts.google.com/o/oauth2/auth?client_id=564921029129.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fgd-ocaml-auth.appspot.com%2Foauth2callback&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force&state=pUdfmCSvS4iQTSUgRKEErc81JArgOOWqIZEkO4si9mk\")\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libgpm2:amd64.\n",
            "(Reading database ... 155325 files and directories currently installed.)\n",
            "Preparing to unpack .../libgpm2_1.20.7-5_amd64.deb ...\n",
            "Unpacking libgpm2:amd64 (1.20.7-5) ...\n",
            "Selecting previously unselected package w3m.\n",
            "Preparing to unpack .../w3m_0.5.3-36build1_amd64.deb ...\n",
            "Unpacking w3m (0.5.3-36build1) ...\n",
            "Setting up libgpm2:amd64 (1.20.7-5) ...\n",
            "Setting up w3m (0.5.3-36build1) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "/content\n",
            "/content/drive\n",
            "/content\n",
            "/\n",
            "Error: Mountpoint /content/drive/MyDrive should be an existing directory.\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive # Link your drive if you are a colab user\n",
        "# drive.mount('/content/drive') # Models in this HW take a long time to get trained and make sure to save it her\n",
        "\n",
        "import os.path as path \n",
        "if not path.exists(\"/content/drive\"):\n",
        "    !sudo add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "    !sudo apt-get update -qq 2>&1 > /dev/null\n",
        "    !sudo apt -y install -qq google-drive-ocamlfuse 2>&1 > /dev/null\n",
        "    !google-drive-ocamlfuse\n",
        "\n",
        "    !sudo apt-get install -qq w3m # to act as web browser \n",
        "    !xdg-settings set default-web-browser w3m.desktop # to set default browser\n",
        "    %cd /content\n",
        "    !mkdir drive\n",
        "    %cd drive\n",
        "#     !mkdir MyDrive\n",
        "    %cd ..\n",
        "    %cd ..\n",
        "    !google-drive-ocamlfuse /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ORNHnSFroP0"
      },
      "source": [
        "# Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "k0v7wHRWrqH6"
      },
      "outputs": [],
      "source": [
        "# ARPABET PHONEME MAPPING\n",
        "# DO NOT CHANGE\n",
        "# This overwrites the phonetics.py file.\n",
        "\n",
        "CMUdict_ARPAbet = {\n",
        "    \"\" : \" \",\n",
        "    \"[SIL]\": \"-\", \"NG\": \"G\", \"F\" : \"f\", \"M\" : \"m\", \"AE\": \"@\", \n",
        "    \"R\"    : \"r\", \"UW\": \"u\", \"N\" : \"n\", \"IY\": \"i\", \"AW\": \"W\", \n",
        "    \"V\"    : \"v\", \"UH\": \"U\", \"OW\": \"o\", \"AA\": \"a\", \"ER\": \"R\", \n",
        "    \"HH\"   : \"h\", \"Z\" : \"z\", \"K\" : \"k\", \"CH\": \"C\", \"W\" : \"w\", \n",
        "    \"EY\"   : \"e\", \"ZH\": \"Z\", \"T\" : \"t\", \"EH\": \"E\", \"Y\" : \"y\", \n",
        "    \"AH\"   : \"A\", \"B\" : \"b\", \"P\" : \"p\", \"TH\": \"T\", \"DH\": \"D\", \n",
        "    \"AO\"   : \"c\", \"G\" : \"g\", \"L\" : \"l\", \"JH\": \"j\", \"OY\": \"O\", \n",
        "    \"SH\"   : \"S\", \"D\" : \"d\", \"AY\": \"Y\", \"S\" : \"s\", \"IH\": \"I\",\n",
        "    \"[SOS]\": \"[SOS]\", \"[EOS]\": \"[EOS]\"}\n",
        "\n",
        "    \n",
        "\n",
        "CMUdict = list(CMUdict_ARPAbet.keys())\n",
        "ARPAbet = list(CMUdict_ARPAbet.values())\n",
        "\n",
        "\n",
        "PHONEMES = CMUdict\n",
        "mapping = CMUdict_ARPAbet\n",
        "LABELS = ARPAbet[:-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eN2kcxwXLLBb"
      },
      "outputs": [],
      "source": [
        "# You might want to play around with the mapping as a sanity check here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agmNBKf4JrLV"
      },
      "source": [
        "### Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "afd0_vlbJmr_"
      },
      "outputs": [],
      "source": [
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    # For this homework, we give you full flexibility to design your data set class.\n",
        "    # Hint: The data from HW1 is very similar to this HW\n",
        "\n",
        "    #TODO\n",
        "    def __init__(self,data_path): \n",
        "        '''\n",
        "        Initializes the dataset.\n",
        "        INPUTS: What inputs do you need here?\n",
        "        '''\n",
        "\n",
        "        self.data_path = data_path\n",
        "        self.mfcc_dir = self.data_path + '/mfcc/'\n",
        "        self.transcript_dir = self.data_path + 'transcript/raw/'\n",
        "\n",
        "        mfcc_names = sorted(os.listdir(self.mfcc_dir))\n",
        "        transcript_names = sorted(os.listdir(self.transcript_dir))\n",
        "\n",
        "        assert len(mfcc_names) == len(transcript_names)\n",
        "\n",
        "        self.mfcc, self.transcript = [], []\n",
        "\n",
        "        # num_examples = int(len(self.mfcc)*percent_data/100) . # Toy Dataset creation\n",
        "\n",
        "        self.PHONEMES = PHONEMES\n",
        "\n",
        "        # TODO:\n",
        "        # Iterate through mfccs and transcripts\n",
        "        for i in range(0, len(mfcc_names)):\n",
        "            mfcc = np.load(self.mfcc_dir + mfcc_names[i], allow_pickle=True)\n",
        "            # print(mfcc[i])\n",
        "        #   Optionally do Cepstral Normalization of mfcc\n",
        "            mfcc = (mfcc - mfcc.mean(axis=0))/mfcc.std(axis=0)\n",
        "        #   Load the corresponding transcript\n",
        "            transcript = np.load(self.transcript_dir + transcript_names[i],allow_pickle=True)[1:-1] # Remove [SOS] and [EOS] from the transcript (Is there an efficient way to do this\n",
        "            # without traversing through the transcript)\n",
        "        #   Append each mfcc to self.mfcc, transcript to self.transcript\n",
        "            self.mfcc.append(mfcc)\n",
        "            self.transcript.append(transcript)\n",
        "\n",
        "        #TODO\n",
        "        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n",
        "        self.length = len(self.transcript)\n",
        "        \n",
        "        #TODO\n",
        "        # HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?\n",
        "        # HINT: TENSORS CANNOT STORE NON-NUMERICAL VALUES OR STRINGS\n",
        "\n",
        "        #TODO\n",
        "        # CREATE AN ARRAY OF ALL FEATUERS AND LABELS\n",
        "        # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n",
        "        '''\n",
        "        You may decide to do this in __getitem__ if you wish.\n",
        "        However, doing this here will make the __init__ function take the load of\n",
        "        loading the data, and shift it away from training.\n",
        "        '''\n",
        "       \n",
        "    def __len__(self):\n",
        "        \n",
        "        '''\n",
        "        TODO: What do we return here?\n",
        "        '''\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        '''\n",
        "        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
        "\n",
        "        If you didn't do the loading and processing of the data in __init__,\n",
        "        do that here.\n",
        "\n",
        "        Once done, return a tuple of features and labels.\n",
        "        '''\n",
        "        \n",
        "        mfcc = torch.FloatTensor(self.mfcc[ind]) # Convert to Tensors\n",
        "        # transcript = torch.tensor([self.PHONEMES.index(i) for i in self.transcript[ind]], dtype=torch.long)\n",
        "        transcript = torch.LongTensor([self.PHONEMES.index(i) for i in self.transcript[ind]])\n",
        "        \n",
        "        return mfcc, transcript\n",
        "\n",
        "    def collate_fn(self,batch):\n",
        "        '''\n",
        "        TODO:\n",
        "        1.  Extract the features and labels from 'batch'\n",
        "        2.  We will additionally need to pad both features and labels,\n",
        "            look at pytorch's docs for pad_sequence\n",
        "        3.  This is a good place to perform transforms, if you so wish. \n",
        "            Performing them on batches will speed the process up a bit.\n",
        "        4.  Return batch of features, labels, lenghts of features, \n",
        "            and lengths of labels.\n",
        "        '''\n",
        "        # batch of input mfcc coefficients\n",
        "        batch_mfcc = [i for i,j in batch]\n",
        "        # batch of outputututututut phonemes\n",
        "        batch_transcript = [j for i,j in batch]\n",
        "\n",
        "        # HINT: CHECK OUT -> pad_sequence (imported above)\n",
        "        # Also be sure to check the input format (batch_first)\n",
        "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first = True)\n",
        "        # lengths_mfcc = [len(m) for m in batch_mfcc] \n",
        "        lengths_mfcc = [m.shape[0] for m in batch_mfcc]\n",
        "\n",
        "        batch_transcript_pad = pad_sequence(batch_transcript, batch_first = True)\n",
        "        #lengths_transcript = [len(t) for t in batch_transcript] \n",
        "        lengths_transcript = [t.shape[0] for t in batch_transcript]\n",
        "\n",
        "        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n",
        "        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n",
        "        #                  -> Would we apply transformation on the validation set as well?\n",
        "        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
        "        \n",
        "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
        "        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)\n",
        "\n",
        "       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqDrxeHfJw4g"
      },
      "source": [
        "### Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HrLS1wfVJppA"
      },
      "outputs": [],
      "source": [
        "# Test Dataloader\n",
        "#TODO\n",
        "class AudioDatasetTest(torch.utils.data.Dataset):\n",
        "    \n",
        "  # Load the directory and all files in them\n",
        "    def __init__(self,data_path):\n",
        "\n",
        "        self.data_path = data_path\n",
        "        self.mfcc_dir = self.data_path + '/mfcc/' \n",
        "\n",
        "        mfcc_names = sorted(os.listdir(self.mfcc_dir))\n",
        "\n",
        "        self.mfcc = [] \n",
        "\n",
        "        self.PHONEMES = PHONEMES\n",
        "\n",
        "\n",
        "        # TODO:\n",
        "        # Iterate through mfccs and transcripts\n",
        "        for i in range(0, len(mfcc_names)):\n",
        "          mfcc = np.load(self.mfcc_dir + mfcc_names[i], allow_pickle = True)\n",
        "          # print(mfcc[i])\n",
        "        # Optionally do Cepstral Normalization of mfcc\n",
        "          mfcc = (mfcc - mfcc.mean(axis=0))/mfcc.std(axis=0)\n",
        "        # Append each mfcc to self.mfcc\n",
        "          self.mfcc.append(mfcc)\n",
        "\n",
        "        self.length = len(self.mfcc)\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "\n",
        "        mfcc = torch.FloatTensor(self.mfcc[ind]) # Convert to Tensors\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "    def collate_fn(self,batch):\n",
        "\n",
        "        batch_mfcc = [i for i in batch] \n",
        "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first = True) \n",
        "        # lengths_mfcc = [len(b) for b in batch_mfcc]\n",
        "        lengths_mfcc = [b.shape[0] for b in batch_mfcc]\n",
        "\n",
        "        return batch_mfcc_pad, torch.tensor(lengths_mfcc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt-veYcdL6Fe"
      },
      "source": [
        "### Data - Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4icymeX1ImUN"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32 # Increase if your device can handle it\n",
        "\n",
        "transforms = [] # set of tranformations\n",
        "# You may pass this as a parameter to the dataset class above\n",
        "# This will help modularize your implementation\n",
        "\n",
        "# root = '/content/hw3p2' "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmuPk9J6L8dz"
      },
      "source": [
        "### Data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_kG0gU2x4hH",
        "outputId": "8ffb5117-7ce6-4d2d-b471-ae3f12e5282e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "153"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get me RAMMM!!!! \n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mzoYfTKu14s",
        "outputId": "bd26b5be-9275-468e-c333-5eeab5587b3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size:  32\n",
            "Train dataset samples = 104014, batches = 3251\n",
            "Val dataset samples = 2703, batches = 85\n",
            "Test dataset samples = 2620, batches = 82\n"
          ]
        }
      ],
      "source": [
        "# Create objects for the dataset class\n",
        "# train_data = AudioDataset('/content/hw3p2/train-clean-100/') # Low Cut-off\n",
        "train_data = AudioDataset('/content/hw3p2/train-clean-360/')\n",
        "val_data = AudioDataset('/content/hw3p2/dev-clean/')\n",
        "test_data = AudioDatasetTest('/content/hw3p2/test-clean/')\n",
        "\n",
        "# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train_data, collate_fn=train_data.collate_fn,\n",
        "                                           batch_size=BATCH_SIZE, pin_memory= True,\n",
        "                                           shuffle= True, num_workers= 4) \n",
        "val_loader = torch.utils.data.DataLoader(val_data, collate_fn=val_data.collate_fn,\n",
        "                                           batch_size=BATCH_SIZE, pin_memory= True,\n",
        "                                           shuffle= False, num_workers= 2)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, collate_fn=test_data.collate_fn,\n",
        "                                           batch_size=BATCH_SIZE, pin_memory= True,\n",
        "                                           shuffle= False, num_workers= 2)\n",
        "\n",
        "print(\"Batch size: \", BATCH_SIZE)\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXMtwyviKaxK",
        "outputId": "4f6b75c1-1ba2-4e72-aac4-21f8a0589fe7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 1640, 15]) torch.Size([32, 204]) torch.Size([32]) torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "for data in train_loader:\n",
        "    x, y, lx, ly = data\n",
        "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
        "    break "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly4mjUUUuJhy"
      },
      "source": [
        "# Model Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZ-qQ_Sf-LIu",
        "outputId": "d7fb9463-85c1-47f5-9617-90267d9fef42"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "OUT_SIZE = len(LABELS)\n",
        "OUT_SIZE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLad4pChcuvX"
      },
      "source": [
        "## Basic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pU6jQ72GmA5D"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class LockedDropout(nn.Module):\n",
        "    \"\"\" LockedDropout applies the same dropout mask to every time step.\n",
        "\n",
        "    **Thank you** to Sales Force for their initial implementation of :class:`WeightDrop`. Here is\n",
        "    their `License\n",
        "    <https://github.com/salesforce/awd-lstm-lm/blob/master/LICENSE>`__.\n",
        "\n",
        "    Args:\n",
        "        p (float): Probability of an element in the dropout mask to be zeroed.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (:class:`torch.FloatTensor` [sequence length, batch size, rnn hidden size]): Input to\n",
        "                apply dropout too.\n",
        "        \"\"\"\n",
        "        if not self.training or not self.p:\n",
        "            return x\n",
        "        x = x.clone()\n",
        "        mask = x.new_empty(1, x.size(1), x.size(2), requires_grad=False).bernoulli_(1 - self.p)\n",
        "        mask = mask.div_(1 - self.p)\n",
        "        mask = mask.expand_as(x)\n",
        "        return x * mask\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "HaksWaQCAPWA"
      },
      "outputs": [],
      "source": [
        "# torch.cuda.empty_cache()\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# class Network(nn.Module):\n",
        "\n",
        "#     def __init__(self, input_size, embed_dim, hidden_dim, out_size, dropout_rate):\n",
        "\n",
        "#         super(Network, self).__init__()\n",
        "\n",
        "#         # Adding some sort of embedding layer or feature extractor might help performance.\n",
        "#         self.embedding = nn.Sequential(nn.Conv1d(in_channels = input_size, out_channels = embed_dim, bias = False, kernel_size = 3, padding = 1, stride = 1),\n",
        "#                                        nn.BatchNorm1d(embed_dim),          \n",
        "#                                                   )\n",
        "\n",
        "#         # TODO : look up the documentation. You might need to pass some additional parameters.\n",
        "#         self.lstm = nn.LSTM(input_size = embed_dim, hidden_size = hidden_dim, num_layers = 3, bidirectional = True) \n",
        "       \n",
        "#         #droupout1d !!\n",
        "#         self.lstm_dropout = LockedDropout(p = dropout_rate)\n",
        "\n",
        "#         self.classification = nn.Sequential(\n",
        "#             nn.Dropout(p=dropout_rate),\n",
        "#             nn.Linear(hidden_dim*2, out_size),\n",
        "#             #TODO: Linear layer with in_features from the lstm module above and out_features = OUT_SIZE\n",
        "#         )\n",
        "        \n",
        "#         # self.classification.appply(self.init_weights)\n",
        "#         # self.lstm.apply(self.init_weights)\n",
        "\n",
        "#         self.logSoftmax = nn.LogSoftmax(dim = 2) #TODO: Apply a log softmax here. Which dimension would apply it on ?\n",
        "\n",
        "#     def init_weights(self, m):\n",
        "#       if isinstance(m,torch.nn.Linear):\n",
        "#         torch.nn.init.xavier_uniform_(m.weight)\n",
        "#       if isinstance(m,torch.nn.Conv1d):\n",
        "#         torch.nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "#     def forward(self, x, lx):\n",
        "#         #TODO\n",
        "#         # The forward function takes 2 parameter inputs here. Why?\n",
        "#         # Refer to the handout for hints\n",
        "        \n",
        "#         out = x.permute((0,2,1))\n",
        "#         out = self.embedding(out)\n",
        "#         out = out.permute((0,2,1))\n",
        "\n",
        "#         out = self.lstm_dropout(out)\n",
        "        \n",
        "#         packed_input = pack_padded_sequence(out, lx, enforce_sorted=False, batch_first = True)\n",
        "\n",
        "#         lstm_out, hidden_dims = self.lstm(packed_input)\n",
        "        \n",
        "\n",
        "#         lstm_pad_pack, lx  = pad_packed_sequence(lstm_out, batch_first = True)\n",
        "\n",
        "#         out = self.classification(lstm_pad_pack)\n",
        "#         out = self.logSoftmax(out)\n",
        "            \n",
        "#         out = out.permute((1,0,2))\n",
        "\n",
        "#         return out, lx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1nCUsMMo8IW2"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Network(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, embed_dim, hidden_dim, out_size, dropout_rate):\n",
        "\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Sequential(nn.Conv1d(in_channels = input_size, out_channels = embed_dim, bias = False, kernel_size = 1, padding = 0, stride = 1),\n",
        "                                       nn.BatchNorm1d(embed_dim),\n",
        "                                       nn.GELU(),\n",
        "                                       nn.Conv1d(in_channels = embed_dim, out_channels = embed_dim, bias = False, kernel_size = 3, padding = 1, stride = 1, groups = embed_dim),\n",
        "                                       nn.BatchNorm1d(embed_dim),\n",
        "                                       nn.GELU(),\n",
        "                                      #  nn.Conv1d(in_channels = embed_dim, out_channels = embed_dim, bias = False, kernel_size = 1, padding = 0, stride = 1),\n",
        "                                      #  nn.BatchNorm1d(embed_dim),\n",
        "                                      #  nn.GELU(),\n",
        "                                       nn.Dropout(0.2)\n",
        "                                                  )\n",
        "\n",
        "        # TODO : look up the documentation. You might need to pass some additional parameters.\n",
        "        self.lstm = nn.LSTM(input_size = embed_dim, hidden_size = hidden_dim, num_layers = 2, bidirectional = True) \n",
        "       \n",
        "        #droupout1d !!\n",
        "\n",
        "        self.classification = nn.Sequential(\n",
        "            nn.Linear((hidden_dim * 2), 2048),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(2048, 41)\n",
        "            #TODO: Linear layer with in_features from the lstm module above and out_features = OUT_SIZE\n",
        "        )\n",
        "        \n",
        "        self.logSoftmax = nn.LogSoftmax(dim = 2) #TODO: Apply a log softmax here. Which dimension would apply it on ?\n",
        "\n",
        "    def forward(self, x, lx):\n",
        "        #TODO\n",
        "        # The forward function takes 2 parameter inputs here. Why?\n",
        "        # Refer to the handout for hints\n",
        "        \n",
        "        out = torch.permute(x, (0,2,1))\n",
        "        out = self.embedding(out)\n",
        "        out = torch.permute(out, (0,2,1))\n",
        "        \n",
        "        packed_input = pack_padded_sequence(out, lx, enforce_sorted=False, batch_first = True)\n",
        "\n",
        "        lstm_out, hidden_dims = self.lstm(packed_input)\n",
        "        \n",
        "\n",
        "        lstm_pad_pack, lx  = pad_packed_sequence(lstm_out, batch_first = True)\n",
        "\n",
        "        out = self.classification(lstm_pad_pack)\n",
        "        out = self.logSoftmax(out)\n",
        "            \n",
        "        out = torch.permute(out, (1,0,2))\n",
        "\n",
        "        return out, lx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUThsowyQdN7"
      },
      "source": [
        "## INIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RRkOUVbDAYYg",
        "outputId": "ab6a8c59-2402-4932-a487-04ad52c90d42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================================\n",
            "                             Kernel Shape      Output Shape     Params  \\\n",
            "Layer                                                                    \n",
            "0_embedding.Conv1d_0         [15, 128, 1]   [32, 128, 1640]      1.92k   \n",
            "1_embedding.BatchNorm1d_1           [128]   [32, 128, 1640]      256.0   \n",
            "2_embedding.GELU_2                      -   [32, 128, 1640]          -   \n",
            "3_embedding.Conv1d_3          [1, 128, 3]   [32, 128, 1640]      384.0   \n",
            "4_embedding.BatchNorm1d_4           [128]   [32, 128, 1640]      256.0   \n",
            "5_embedding.GELU_5                      -   [32, 128, 1640]          -   \n",
            "6_embedding.Dropout_6                   -   [32, 128, 1640]          -   \n",
            "7_lstm                                  -      [38939, 512]  2.367488M   \n",
            "8_classification.Linear_0     [512, 2048]  [32, 1640, 2048]  1.050624M   \n",
            "9_classification.GELU_1                 -  [32, 1640, 2048]          -   \n",
            "10_classification.Dropout_2             -  [32, 1640, 2048]          -   \n",
            "11_classification.Linear_3     [2048, 41]    [32, 1640, 41]    84.009k   \n",
            "12_logSoftmax                           -    [32, 1640, 41]          -   \n",
            "\n",
            "                             Mult-Adds  \n",
            "Layer                                   \n",
            "0_embedding.Conv1d_0           3.1488M  \n",
            "1_embedding.BatchNorm1d_1        128.0  \n",
            "2_embedding.GELU_2                   -  \n",
            "3_embedding.Conv1d_3           629.76k  \n",
            "4_embedding.BatchNorm1d_4        128.0  \n",
            "5_embedding.GELU_5                   -  \n",
            "6_embedding.Dropout_6                -  \n",
            "7_lstm                       2.359296M  \n",
            "8_classification.Linear_0    1.048576M  \n",
            "9_classification.GELU_1              -  \n",
            "10_classification.Dropout_2          -  \n",
            "11_classification.Linear_3     83.968k  \n",
            "12_logSoftmax                        -  \n",
            "---------------------------------------------------------------------------------\n",
            "                         Totals\n",
            "Total params          3.504937M\n",
            "Trainable params      3.504937M\n",
            "Non-trainable params        0.0\n",
            "Mult-Adds             7.270656M\n",
            "=================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-93a682db-a315-45ce-bcc2-4cedd21f1dc3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_embedding.Conv1d_0</th>\n",
              "      <td>[15, 128, 1]</td>\n",
              "      <td>[32, 128, 1640]</td>\n",
              "      <td>1920.0</td>\n",
              "      <td>3148800.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_embedding.BatchNorm1d_1</th>\n",
              "      <td>[128]</td>\n",
              "      <td>[32, 128, 1640]</td>\n",
              "      <td>256.0</td>\n",
              "      <td>128.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_embedding.GELU_2</th>\n",
              "      <td>-</td>\n",
              "      <td>[32, 128, 1640]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_embedding.Conv1d_3</th>\n",
              "      <td>[1, 128, 3]</td>\n",
              "      <td>[32, 128, 1640]</td>\n",
              "      <td>384.0</td>\n",
              "      <td>629760.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_embedding.BatchNorm1d_4</th>\n",
              "      <td>[128]</td>\n",
              "      <td>[32, 128, 1640]</td>\n",
              "      <td>256.0</td>\n",
              "      <td>128.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5_embedding.GELU_5</th>\n",
              "      <td>-</td>\n",
              "      <td>[32, 128, 1640]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6_embedding.Dropout_6</th>\n",
              "      <td>-</td>\n",
              "      <td>[32, 128, 1640]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7_lstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[38939, 512]</td>\n",
              "      <td>2367488.0</td>\n",
              "      <td>2359296.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8_classification.Linear_0</th>\n",
              "      <td>[512, 2048]</td>\n",
              "      <td>[32, 1640, 2048]</td>\n",
              "      <td>1050624.0</td>\n",
              "      <td>1048576.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9_classification.GELU_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[32, 1640, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10_classification.Dropout_2</th>\n",
              "      <td>-</td>\n",
              "      <td>[32, 1640, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11_classification.Linear_3</th>\n",
              "      <td>[2048, 41]</td>\n",
              "      <td>[32, 1640, 41]</td>\n",
              "      <td>84009.0</td>\n",
              "      <td>83968.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12_logSoftmax</th>\n",
              "      <td>-</td>\n",
              "      <td>[32, 1640, 41]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-93a682db-a315-45ce-bcc2-4cedd21f1dc3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-93a682db-a315-45ce-bcc2-4cedd21f1dc3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-93a682db-a315-45ce-bcc2-4cedd21f1dc3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                             Kernel Shape  ...  Mult-Adds\n",
              "Layer                                      ...           \n",
              "0_embedding.Conv1d_0         [15, 128, 1]  ...  3148800.0\n",
              "1_embedding.BatchNorm1d_1           [128]  ...      128.0\n",
              "2_embedding.GELU_2                      -  ...        NaN\n",
              "3_embedding.Conv1d_3          [1, 128, 3]  ...   629760.0\n",
              "4_embedding.BatchNorm1d_4           [128]  ...      128.0\n",
              "5_embedding.GELU_5                      -  ...        NaN\n",
              "6_embedding.Dropout_6                   -  ...        NaN\n",
              "7_lstm                                  -  ...  2359296.0\n",
              "8_classification.Linear_0     [512, 2048]  ...  1048576.0\n",
              "9_classification.GELU_1                 -  ...        NaN\n",
              "10_classification.Dropout_2             -  ...        NaN\n",
              "11_classification.Linear_3     [2048, 41]  ...    83968.0\n",
              "12_logSoftmax                           -  ...        NaN\n",
              "\n",
              "[13 rows x 4 columns]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "model = Network(input_size=15, embed_dim=128,hidden_dim=256,out_size=41,dropout_rate=0.2).to(device)\n",
        "summary(model, x.to(device), lx) # x and lx come from the sanity check above :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NiYtqWP2Od9",
        "outputId": "e6643844-ea2c-4ee7-e1c7-9e1f4b3d518a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "106"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwunYpyugFg"
      },
      "source": [
        "# Training Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "MN82c3KpLup8"
      },
      "outputs": [],
      "source": [
        "train_config = {\n",
        "    \"beam_width\" : 2,\n",
        "    \"lr\" : 2e-3,\n",
        "    \"epochs\" : 25\n",
        "    } # Feel free to add more items here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "iGoozH2nd6KB"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "\n",
        "criterion = torch.nn.CTCLoss()# Define CTC loss as the criterion. How would the losses be reduced?\n",
        "# CTC Loss: https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html\n",
        "# Refer to the handout for hints\n",
        "\n",
        "optimizer =  torch.optim.AdamW(model.parameters(),lr=train_config['lr'],weight_decay=5e-5) # What goes in here?\n",
        "\n",
        "# Declare the decoder. Use the CTC Beam Decoder to decode phonemes\n",
        "# CTC Beam Decoder Doc: https://github.com/parlance/ctcdecode\n",
        "decoder = CTCBeamDecoder(labels=LABELS,beam_width=train_config['beam_width'],num_processes=4,log_probs_input=True)#TODO \n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min', factor=0.5, patience=1,verbose=True)#TODO\n",
        "\n",
        "# Mixed Precision, if you need it\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmc6_4eWL2Xp"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "### Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "KHjnCDddL36E"
      },
      "outputs": [],
      "source": [
        "# Use debug = True to see debug outputs\n",
        "def calculate_levenshtein(h, y, lh, ly, decoder, labels, debug = False):\n",
        "\n",
        "    if debug:\n",
        "        pass\n",
        "        print(f\"\\n----- IN LEVENSHTEIN -----\\n\")\n",
        "        # Add any other debug statements as you may need\n",
        "        # you may want to use debug in several places in this function\n",
        "        \n",
        "    # TODO: look at docs for CTC.decoder and find out what is returned here\n",
        "    # h = h.permute(1,0,2)\n",
        "    h = torch.permute(h,(1,0,2))\n",
        "    beam_results, beam_scores, timesteps, out_seq_len = decoder.decode(h, seq_lens = lh)\n",
        "\n",
        "    batch_size = len(beam_results) # TODO\n",
        "    distance = 0 # Initialize the distance to be 0 initially\n",
        "    \n",
        "    for i in range(len(beam_results)):\n",
        "        # TODO: Loop through each element in the batch\n",
        "        if out_seq_len[i][0] != 0:\n",
        "            decoded_slice = \"\".join([labels[p] for p in beam_results[i,0,:out_seq_len[i,0]]])\n",
        "            target = \"\".join([labels[int(q)] for q in y[i,0:ly[i]]])\n",
        "            d = Levenshtein.distance(decoded_slice,target)\n",
        "            distance += d\n",
        "            # print(distance)\n",
        "            # pass\n",
        "\n",
        "    distance /= len(beam_results) # TODO: Uncomment this, but think about why we are doing this\n",
        "\n",
        "    return distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnTLL-5gMBrY",
        "outputId": "8c78d43e-1e16-4ede-e9f0-cc57c6ad056f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 30.61703872680664\n",
            "lev-distance: 470.875\n"
          ]
        }
      ],
      "source": [
        "# ANOTEHR SANITY CHECK\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i, (out,y,out_lengths,ly) in enumerate(train_loader):\n",
        "      \n",
        "      #TODO: \n",
        "      # Follow the following steps, and \n",
        "      # Add some print statements here for sanity checking\n",
        "      out_tmp, y, out_lengths_tmp, ly = out.to(device), y.to(device), out_lengths, ly\n",
        "      out, out_lengths = model(out, out_lengths)\n",
        "      # print(out.shape,out_lengths.shape)\n",
        "\n",
        "      #1. What values are you returning from the collate function\n",
        "      #2. Move the features and target to <DEVICE>\n",
        "      #3. Print the shapes of each to get a fair understanding \n",
        "      #4. Pass the inputs to the model\n",
        "            # Think of the following before you implement:\n",
        "            # 4.1 What will be the input to your model?\n",
        "            # 4.2 What would the model output?\n",
        "            # 4.3 Print the shapes of the output to get a fair understanding \n",
        "\n",
        "      # Calculate loss: https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html\n",
        "      # Calculating the loss is not straightforward. Check the input format of each parameter\n",
        "      \n",
        "      loss = criterion(out,y,out_lengths,ly) # What goes in here?\n",
        "      print(f\"loss: {loss}\")\n",
        "\n",
        "      distance = calculate_levenshtein(out, y, out_lengths, ly, decoder, LABELS, debug = False)\n",
        "      print(f\"lev-distance: {distance}\")\n",
        "\n",
        "      break # one iteration is enough"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fLLj5KIMMOe"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH0RAbCaMl9a"
      },
      "source": [
        "### Eval function\n",
        "Writing a function to do one round of evaluations will help make your code more modular, you can, however, choose to skip this if you'd like it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "0nqLiAmkMMBc"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "def evaluate(data_loader, model):\n",
        "    model.eval()\n",
        "    dist = 0\n",
        "    loss = 0\n",
        "    batch_bar = tqdm(total=len(data_loader), dynamic_ncols=True, leave=False, position=0, desc='Val') \n",
        "    # TODO Fill this function out, if you're using it.\n",
        "    for i, (h,y,lh,ly) in enumerate(data_loader):\n",
        "        h,y,lh,ly = h.to(device),y.to(device),lh,ly\n",
        "        \n",
        "        with torch.inference_mode():\n",
        "            out, out_length = model(h,lh)\n",
        "            l = criterion(out,y,out_length,ly)\n",
        "            d = calculate_levenshtein(out,y,out_length,ly,decoder,LABELS,debug=False)\n",
        "        \n",
        "        batch_bar.set_postfix(loss = f\"{loss/ (i+1):.4f}\", Distance = f\"{dist/(i+1)}\")\n",
        "        \n",
        "        loss+=l\n",
        "        dist+=d\n",
        "        \n",
        "        batch_bar.update()\n",
        "        \n",
        "    batch_bar.close()\n",
        "    del out, out_length, lh, ly, h, y\n",
        "\n",
        "    loss /=len(data_loader)\n",
        "    dist /=len(data_loader)\n",
        "    \n",
        "    \n",
        "    print(f\"\\n Validation Loss: {loss:.4f}\")\n",
        "    print(f\"\\n Distance: {dist:.4f}\")\n",
        "    \n",
        "    return loss, dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpYExu4vT4_g"
      },
      "source": [
        "### Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "tExvyl1BIdMC"
      },
      "outputs": [],
      "source": [
        "# This is for checkpointing, if you're doing it over multiple sessions\n",
        "\n",
        "last_epoch_completed = 0\n",
        "start = last_epoch_completed\n",
        "end = train_config['epochs']\n",
        "best_val_dist = float(\"inf\") # if you're restarting from some checkpoint, use what you saw there.\n",
        "dist_freq = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGn17rLw9ChF"
      },
      "source": [
        "Again, writing a train step might help you code be more modular. You may choose to skip this and write the whole thing out in the training loop below if you so wish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "_vH4QStLUjH8"
      },
      "outputs": [],
      "source": [
        "def train_step(train_loader, model, optimizer, criterion, scheduler, scaler):\n",
        "    \n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "    train_loss = 0\n",
        "    model.train()\n",
        "\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "        h, y, lh, ly = data\n",
        "        optimizer.zero_grad()\n",
        "        h, y = h.to(device), y.to(device)\n",
        "\n",
        "        # TODO: Fill this with the help of your sanity check\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            out,out_length = model(h,lh)\n",
        "            loss = criterion(out,y,out_length,ly)\n",
        "\n",
        "        # HINT: Are you using mixed precision? \n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss = f\"{train_loss/ (i+1):.4f}\",\n",
        "            lr = f\"{optimizer.param_groups[0]['lr']}\"\n",
        "        )\n",
        "        \n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        train_loss += loss\n",
        "        batch_bar.update()\n",
        "    \n",
        "    batch_bar.close()\n",
        "    del out, out_length, lh, ly, h, y\n",
        "\n",
        "    train_loss /= len(train_loader) # TODO\n",
        "    print(f\"\\n Training Loss: {loss:.4f}\")\n",
        "\n",
        "    return train_loss # And anything else you may wish to get out of this function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY69hgxUXhTI"
      },
      "source": [
        "### Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "#TODO: Please complete the training loop\n",
        "\n",
        "for epoch in range(train_config[\"epochs\"]):\n",
        "\n",
        "    # one training step\n",
        "    # one validation step (if you want)\n",
        "\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch+1,train_config[\"epochs\"]))\n",
        "\n",
        "    # HINT: Calculating levenshtein distance takes a long time. Do you need to do it every epoch?\n",
        "    # Does the training step even need it? \n",
        "\n",
        "    train_loss = train_step(train_loader, model,optimizer, criterion, scheduler, scaler)\n",
        "    val_loss, val_dist = evaluate(val_loader, model)\n",
        "\n",
        "    # Where you have your scheduler.step depends on the scheduler you use.\n",
        "    scheduler.step(val_dist)\n",
        "    \n",
        "    \n",
        "    # Use the below code to save models\n",
        "    if val_dist < best_val_dist:\n",
        "      #path = os.path.join(root_path, model_directory, 'checkpoint' + '.pth')\n",
        "      # print(\"Saving model\")\n",
        "      # torch.save({'model_state_dict':model.state_dict(),\n",
        "      #             'optimizer_state_dict':optimizer.state_dict(),\n",
        "      #             'train_loss': train_loss,\n",
        "      #             'val_dist': val_dist, \n",
        "      #             'epoch': epoch}, \n",
        "      #             '/content/gdrive/MyDrive/hw3p2-checkpoint-ak.pth')\n",
        "      best_val_dist = val_dist\n",
        "#       wandb.save('checkpoint.pth')\n",
        "    \n",
        "\n",
        "    # You may want to log some hyperparameters and results on wandb\n",
        "#     wandb.log()\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Extra Training\n",
        "High Cutoff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR43E28rM9Ak",
        "outputId": "83617332-9467-4a77-ff91-04b58eac7b9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Training Loss: 0.1243\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Validation Loss: 0.2433\n",
            "\n",
            " Distance: 4.8894\n",
            "Epoch    26: reducing learning rate of group 0 to 2.5000e-04.\n",
            "\n",
            "Epoch 2/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Training Loss: 0.1211\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Validation Loss: 0.2372\n",
            "\n",
            " Distance: 4.7538\n",
            "\n",
            "Epoch 3/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Training Loss: 0.1780\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Validation Loss: 0.2379\n",
            "\n",
            " Distance: 4.7695\n",
            "\n",
            "Epoch 4/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Training Loss: 0.1150\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Validation Loss: 0.2381\n",
            "\n",
            " Distance: 4.7460\n",
            "\n",
            "Epoch 5/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Training Loss: 0.1463\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Validation Loss: 0.2382\n",
            "\n",
            " Distance: 4.7257\n",
            "\n",
            "Epoch 6/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Training Loss: 0.1419\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Validation Loss: 0.2384\n",
            "\n",
            " Distance: 4.7584\n",
            "\n",
            "Epoch 7/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Training Loss: 0.1958\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Validation Loss: 0.2376\n",
            "\n",
            " Distance: 4.7400\n",
            "Epoch    32: reducing learning rate of group 0 to 1.2500e-04.\n",
            "\n",
            "Epoch 8/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Training Loss: 0.1667\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Validation Loss: 0.2377\n",
            "\n",
            " Distance: 4.7094\n",
            "\n",
            "Epoch 9/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Training Loss: 0.1272\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Validation Loss: 0.2380\n",
            "\n",
            " Distance: 4.6906\n",
            "\n",
            "Epoch 10/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Training Loss: 0.1105\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                             "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Validation Loss: 0.2374\n",
            "\n",
            " Distance: 4.6531\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "#TODO: Please complete the training loop\n",
        "\n",
        "for epoch in range(10):#train_config[\"epochs\"]): # Note additional training is for 10 epochs\n",
        "\n",
        "    # one training step\n",
        "    # one validation step (if you want)\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch+1,train_config[\"epochs\"]))  # forgot to replace config[\"epochs\"] with 10 -> printed as 10/25 in logs\n",
        "    # HINT: Calculating levenshtein distance takes a long time. Do you need to do it every epoch?\n",
        "    # Does the training step even need it? \n",
        "    train_loss = train_step(train_loader,model,optimizer,criterion,scheduler,scaler)\n",
        "    val_loss, val_dist = evaluate(val_loader,model)\n",
        "    \n",
        "    scheduler.step(val_dist)\n",
        "    # Where you have your scheduler.step depends on the scheduler you use.\n",
        "    \n",
        "    \n",
        "    # Use the below code to save models\n",
        "    if val_dist < best_val_dist:\n",
        "      #path = os.path.join(root_path, model_directory, 'checkpoint' + '.pth')\n",
        "      # print(\"Saving model\")\n",
        "      # torch.save({'model_state_dict':model.state_dict(),\n",
        "      #             'optimizer_state_dict':optimizer.state_dict(),\n",
        "      #             'train_loss': train_loss,\n",
        "      #             'val_dist': val_dist, \n",
        "      #             'epoch': epoch}, \n",
        "      #             '/content/gdrive/MyDrive/hw3p2-checkpoint-ak.pth')\n",
        "      best_val_dist = val_dist\n",
        "#       wandb.save('checkpoint.pth')\n",
        "    \n",
        "\n",
        "    # You may want to log some hyperparameters and results on wandb\n",
        "#     wandb.log()\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2H4EEj-sD32"
      },
      "source": [
        "# Generate Predictions and Submit to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "2moYJhTWsOG-"
      },
      "outputs": [],
      "source": [
        "#TODO: Make predictions\n",
        "\n",
        "# Follow the steps below:\n",
        "# 1. Create a new object for CTCBeamDecoder with larger (why?) number of beams\n",
        "# 2. Get prediction string by decoding the results of the beam decoder\n",
        "\n",
        "decoder_test = CTCBeamDecoder(labels = LABELS, beam_width = 10, num_processes = 4, log_probs_input = True)\n",
        "\n",
        "def make_output(h, lh, decoder, LABELS):\n",
        "\n",
        "    h = torch.permute(h,(1,0,2))\n",
        "    # print(h.shape)\n",
        "    beam_results, beam_scores, timesteps, out_seq_len = decoder_test.decode(h, seq_lens=lh) #TODO: What parameters would the decode function take in?\n",
        "    batch_size = len(beam_results) #What is the batch size\n",
        "\n",
        "    dist = 0\n",
        "    preds = []\n",
        "    for i in range(batch_size): # Loop through each element in the batch\n",
        "        if out_seq_len[i,0] != 0:\n",
        "          # h_sliced = #TODO: Obtain the beam results\n",
        "          h_string = \"\".join([LABELS[b] for b in beam_results[i,0,:out_seq_len[i,0]]])\n",
        "          preds.append(h_string)\n",
        "    \n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "R_JfVPf-MiHL"
      },
      "outputs": [],
      "source": [
        "def predict(data_loader,model,decoder,debug=False):\n",
        "  model.eval()\n",
        "  pred=[]\n",
        "  for i, data in enumerate(data_loader):\n",
        "    x,lx = data\n",
        "    x = x.to(device)\n",
        "    output, l = model(x, lx)\n",
        "    \n",
        "    predictions = make_output(output, lx, decoder, LABELS)\n",
        "    for j in predictions:\n",
        "      pred.append(j)\n",
        "  return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRL77zgdSQXI",
        "outputId": "ed157105-61f0-4313-ce52-ab09e7da44bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 825, 15]) torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "for data in test_loader:\n",
        "    x, lx = data\n",
        "    print(x.shape, lx.shape)\n",
        "    break "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d70dvu_lsMlv",
        "outputId": "6a5b630f-bdac-4a00-ac32-06ba766519e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.8)\n",
            "100% 208k/208k [00:01<00:00, 172kB/s]\n",
            "Successfully submitted to Automatic Speech Recognition (ASR)"
          ]
        }
      ],
      "source": [
        "#TODO:\n",
        "# Write a function (predict) to generate predictions and submit the file to Kaggle\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "predictions = predict(test_loader, model, decoder_test)\n",
        "import pandas as pd\n",
        "\n",
        "# with open(\"submission.csv\", \"w+\") as f:\n",
        "#     f.write(\"index,label\\n\")\n",
        "#     for i in range(len(predictions)):\n",
        "#         f.write(\"{},{}\\n\".format(i, predictions[i]))\n",
        "\n",
        "df = pd.read_csv('/content/hw3p2/test-clean/transcript/random_submission.csv')\n",
        "df.label = predictions\n",
        "\n",
        "df.to_csv('submission.csv', index = False)\n",
        "!kaggle competitions submit -c 11-785-f22-hw3p2 -f submission.csv -m \"I made it!\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Jmc6_4eWL2Xp"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
